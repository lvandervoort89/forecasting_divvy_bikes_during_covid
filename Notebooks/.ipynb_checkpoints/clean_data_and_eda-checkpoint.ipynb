{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Divvy Bike Share Demand During Covid-19\n",
    "**Objective**   \n",
    "Forecast daily Divvy rideshare demand across all Chicago stations from September 1-December 31, 2020 using data from January 1, 2017-August 31, 2020  \n",
    "\n",
    "**Data**   \n",
    "This data was pull from the [City of Chicago](https://divvy-tripdata.s3.amazonaws.com/index.html) from January 1, 2017-August 31, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get data from PostgreSQL and clean. Create dataframes of aggregated data by year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_to_dataframe():\n",
    "    '''\n",
    "    A function that connects to a local PostgreSQL database and queries Divvy\n",
    "    bikeshare into yearly dataframes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dataframe for each year of Divvy data (4 in total).\n",
    "    '''\n",
    "    # Use SQLAlchemy to connect to PostgreSQL database\n",
    "    cnx = create_engine('postgresql://lisavandervoort@localhost:5432/divvy')\n",
    "\n",
    "    # Create dataframes containing each year's individual ride data\n",
    "    all_data_2020 = pd.read_sql_query('''SELECT * FROM jan_feb_march2020\n",
    "                                UNION SELECT * FROM april2020\n",
    "                                UNION SELECT * FROM may2020\n",
    "                                UNION SELECT * FROM june2020\n",
    "                                UNION SELECT * FROM july2020\n",
    "                                UNION SELECT * FROM august2020\n",
    "                            ''', cnx)\n",
    "\n",
    "    all_data_2019 = pd.read_sql_query('''SELECT * FROM jan_feb_march2019\n",
    "                                UNION SELECT * FROM april_may_june2019\n",
    "                                UNION SELECT * FROM july_aug_sept2019\n",
    "                                UNION SELECT * FROM oct_nov_dec2019\n",
    "                            ''', cnx)\n",
    "\n",
    "    all_data_2018 = pd.read_sql_query('''SELECT * FROM jan_feb_march2018\n",
    "                                UNION SELECT * FROM april_may_june2018\n",
    "                                UNION SELECT * FROM july_aug_sept2018\n",
    "                                UNION SELECT * FROM oct_nov_dec2018\n",
    "                            ''', cnx)\n",
    "\n",
    "    all_data_2017 = pd.read_sql_query('''SELECT * FROM jan_feb_march2017\n",
    "                                UNION SELECT * FROM april_may_june2017\n",
    "                                UNION SELECT * FROM july_aug_sept2017\n",
    "                                UNION SELECT * FROM oct_nov_dec2017\n",
    "                            ''', cnx)\n",
    "\n",
    "\n",
    "    return all_data_2020, all_data_2019, all_data_2018, all_data_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_2020, all_data_2019, all_data_2018, all_data_2017 = sql_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_engineer_dataframes(all_data_2020, all_data_2019, all_data_2018, all_data_2017):\n",
    "    '''\n",
    "    A function that cleans the yearly Divvy data and saves the dataframes to csv files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_data_2020, all_data_2019, all_data_2018, all_data_2017 : Divvy bike share data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    This saves each yearly dataframe cleaned as a csv file and a daily ride dataframe as a csv.\n",
    "    '''\n",
    "    ## Clean 2020 dataframe\n",
    "    # Drop nulls\n",
    "    all_data_2020.dropna(inplace=True)\n",
    "\n",
    "    # Drop rows with electric bikes\n",
    "    all_data_2020.drop(\n",
    "        all_data_2020[all_data_2020['rideable_type'] == 'electric_bike'].index, inplace=True)\n",
    "\n",
    "    # Rename columns to match prior years\n",
    "    all_data_2020 = all_data_2020.rename(\n",
    "        columns={'started_at':'start_time', 'start_station_id': 'from_station_id'})\n",
    "\n",
    "    # Create new column with day of year\n",
    "    all_data_2020['start_day_of_year'] = all_data_2020.start_time.dt.date\n",
    "\n",
    "    # Create new column with month\n",
    "    all_data_2020['month'] = all_data_2020.start_time.dt.month\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    all_data_2020 = all_data_2020.drop(\n",
    "        ['ride_id', 'rideable_type', 'ended_at', 'start_station_name',\n",
    "         'end_station_id', 'member_casual', 'end_station_name', 'start_lat', 'start_lng',\n",
    "         'end_lat', 'end_lng'], axis=1)\n",
    "\n",
    "\n",
    "    ## Clean 2019 dataframe\n",
    "    # Create new column with day of year\n",
    "    all_data_2019['start_day_of_year'] = all_data_2019.start_time.dt.date # pylint: disable=no-member\n",
    "\n",
    "    # Create new column with month\n",
    "    all_data_2019['month'] = all_data_2019.start_time.dt.month # pylint: disable=no-member\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    all_data_2019 = all_data_2019.drop(\n",
    "        ['trip_id', 'end_time', 'bikeid', 'tripduration', 'from_station_name',\n",
    "         'to_station_id', 'gender', 'to_station_name', 'birthyear', 'usertype'], axis=1)\n",
    "\n",
    "\n",
    "    ## Clean 2018 dataframe\n",
    "    # Create new column with day of year\n",
    "    all_data_2018['start_day_of_year'] = all_data_2018.start_time.dt.date # pylint: disable=no-member\n",
    "\n",
    "    # Create new column with month\n",
    "    all_data_2018['month'] = all_data_2018.start_time.dt.month # pylint: disable=no-member\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    all_data_2018= all_data_2018.drop(\n",
    "        ['trip_id', 'end_time', 'bikeid', 'tripduration', 'from_station_name',\n",
    "         'to_station_id', 'gender', 'to_station_name', 'birthyear', 'usertype'], axis=1)\n",
    "\n",
    "\n",
    "    ## Clean 2017 dataframe\n",
    "    # Create new column with day of year\n",
    "    all_data_2017['start_day_of_year'] = all_data_2017.start_time.dt.date # pylint: disable=no-member\n",
    "\n",
    "    # Create new column with month\n",
    "    all_data_2017['month'] = all_data_2017.start_time.dt.month # pylint: disable=no-member\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    all_data_2017 = all_data_2017.drop(\n",
    "        ['trip_id', 'end_time', 'bikeid', 'tripduration', 'from_station_name',\n",
    "         'to_station_id', 'gender', 'to_station_name', 'birthyear', 'usertype'], axis=1)\n",
    "\n",
    "    # Concatenate 2017-2020 data together\n",
    "    all_data_2017_to_2020 = pd.concat([all_data_2020, all_data_2019, all_data_2018, all_data_2017], ignore_index=True)\n",
    "\n",
    "    # Group data by day\n",
    "    daily_df_2017_to_2020 = all_data_2017_to_2020.groupby(\n",
    "        ['from_station_id', 'start_day_of_year'], as_index=False).month.count()\n",
    "    daily_df_2017_to_2020 = daily_df_2017_to_2020.rename(\n",
    "        columns={'month': 'number_daily_rides'})\n",
    "\n",
    "    # Save dataframes to csv files\n",
    "    all_data_2020.to_csv(r'all_data_2020.csv', index=False)\n",
    "    all_data_2019.to_csv(r'all_data_2019.csv', index=False)\n",
    "    all_data_2018.to_csv(r'all_data_2018.csv', index=False)\n",
    "    all_data_2017.to_csv(r'all_data_2017.csv', index=False)\n",
    "\n",
    "    # Save daily dataframe\n",
    "    daily_df_2017_to_2020.to_csv(r'daily_df_2017_to_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_and_engineer_dataframes(all_data_2020, all_data_2019, all_data_2018, all_data_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform EDA on Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph daily data\n",
    "daily_df_2017_to_2020.set_index('start_day_of_year', inplace=True)\n",
    "daily_df_2017_to_2020.number_daily_rides.plot(figsize=(18,7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "daily_df_2017_to_2020 = daily_df_2017_to_2020.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert start_day_of_year to datetime object\n",
    "daily_df_2017_to_2020.start_day_of_year = pd.to_datetime(daily_df_2017_to_2020.start_day_of_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month, year, and day of week columns\n",
    "daily_df_2017_to_2020['month'] = daily_df_2017_to_2020.start_day_of_year.dt.month\n",
    "daily_df_2017_to_2020['year'] = daily_df_2017_to_2020.start_day_of_year.dt.year\n",
    "daily_df_2017_to_2020['day_of_week'] = daily_df_2017_to_2020.start_day_of_year.dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph average number of Divvy rides per month over time\n",
    "divvy_monthly = daily_df_2017_to_2020.groupby(['month','year'])['number_daily_rides'].mean().reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(18,6));\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "\n",
    "plt.plot(np.array(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']), np.array(divvy_monthly[divvy_monthly.year==2017].number_daily_rides), color='pink', alpha=0.6, linewidth=5.0)\n",
    "\n",
    "plt.plot(np.array(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']), np.array(divvy_monthly[divvy_monthly.year==2018].number_daily_rides), color='blue', alpha=0.6, linewidth=5.0)\n",
    "\n",
    "plt.plot(np.array(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']), np.array(divvy_monthly[divvy_monthly.year==2019].number_daily_rides), color='red', alpha=0.6, linewidth=5.0)\n",
    "\n",
    "plt.plot(np.array(['Jan','Feb','Mar','Apr','May','Jun','Jul', 'Aug']), np.array(divvy_monthly[divvy_monthly.year==2020].number_daily_rides), color='green', alpha=0.6, linewidth=5.0)\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.title('Average Number of Divvy Rides Per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of rides')\n",
    "plt.legend(['2017', '2018', '2019', '2020']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine rides per day of week by year\n",
    "df_day_of_week_year = daily_df_2017_to_2020.groupby(['day_of_week', 'year'])\n",
    "total_rides_by_day_week_year = df_day_of_week_year.number_daily_rides.sum()\n",
    "total_rides_by_day_week_year = total_rides_by_day_week_year.reset_index()\n",
    "total_rides_by_day_week_year = total_rides_by_day_week_year.sort_values(by=['year', 'number_daily_rides'], ascending=False)\n",
    "total_rides_by_day_week_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Covid column\n",
    "def determine_covid(date):\n",
    "    \"Takes in a date and returns a value of 0 or 1 if the date is before or during covid\"\n",
    "    if date >= datetime.date(year=2020, month=3, day=17):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "daily_df_2017_to_2020['covid'] = daily_df_2017_to_2020.apply(lambda x: determine_covid(x['start_day_of_year']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 2020 demand before covid\n",
    "pre_covid_2020 = daily_df_2017_to_2020[(daily_df_2017_to_2020['covid'] ==0) & (daily_df_2017_to_2020['year'] == 2020)]\n",
    "pre_covid_day_of_week_2020 = pre_covid_2020.groupby('day_of_week')\n",
    "pre_covid_total_rides_by_day_week_2020 = pre_covid_day_of_week_2020.number_daily_rides.sum()\n",
    "pre_covid_total_rides_by_day_week_2020 = pre_covid_total_rides_by_day_week_2020.reset_index()\n",
    "pre_covid_total_rides_by_day_week_2020 = pre_covid_total_rides_by_day_week_2020.sort_values(by=['number_daily_rides'], ascending=False)\n",
    "pre_covid_total_rides_by_day_week_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 2020 demand during covid\n",
    "covid_2020 = daily_df_2017_to_2020[daily_df_2017_to_2020['covid'] ==1]\n",
    "covid_day_of_week_2020 = covid_2020.groupby('day_of_week')\n",
    "covid_total_rides_by_day_week_2020 = covid_day_of_week_2020.number_daily_rides.sum()\n",
    "covid_total_rides_by_day_week_2020 = covid_total_rides_by_day_week_2020.reset_index()\n",
    "covid_total_rides_by_day_week_2020 = covid_total_rides_by_day_week_2020.sort_values(by=['number_daily_rides'], ascending=False)\n",
    "covid_total_rides_by_day_week_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 2019 daily demand (same dates as covid for 2020)\n",
    "start_date = '03-17-2019'\n",
    "end_date = '08-31-2019'\n",
    "mask = (daily_df_2017_to_2020['start_day_of_year'] >= start_date) & (daily_df_2017_to_2020['start_day_of_year'] <= end_date)\n",
    "pre_covid_data_2019 = daily_df_2017_to_2020.loc[mask]\n",
    "precovid_day_of_week_2019 = pre_covid_data_2019.groupby('day_of_week')\n",
    "precovid_total_rides_by_day_week_2019 = precovid_day_of_week_2019.number_daily_rides.sum()\n",
    "precovid_total_rides_by_day_week_2019 = precovid_total_rides_by_day_week_2019.reset_index()\n",
    "precovid_total_rides_by_day_week_2019 = precovid_total_rides_by_day_week_2019.sort_values(by=['number_daily_rides'], ascending=False)\n",
    "precovid_total_rides_by_day_week_2019 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
